###
### Ollama environment configuration
### https://github.com/ollama/ollama/blob/main/envconfig/config.go
### 
PATH="${PATH}"
## Host returns the scheme and host. Host can be configured via the OLLAMA_HOST environment variable.
## Default is scheme "http" and host "127.0.0.1:11434"
# OLLAMA_HOST='http://127.0.0.1:11434'
## AllowedOrigins returns a list of allowed origins. AllowedOrigins can be configured via the OLLAMA_ORIGINS environment variable.
# OLLAMA_ORIGINS='localhost,127.0.0.1,::1,app://*,file://*,tauri://*,vscode-webview://*,vscode-file://*'
## Models returns the path to the models directory. Models directory can be configured via the OLLAMA_MODELS environment variable.
## Default is $HOME/.ollama/models
#OLLAMA_MODELS='$HOME/.ollama/models
## The duration that models stay loaded in memory (default 5 minutes).
## Negative values are treated as infinite. Zero is treated as no keep alive.
#OLLAMA_KEEP_ALIVE='300'
## How long to allow model loads to stall before giving up (default 5 minues)
## LoadTimeout returns the duration for stall detection during model loads. LoadTimeout can be configured via the OLLAMA_LOAD_TIMEOUT environment variable.
## Zero or Negative values are treated as infinite.
## Default is 5 minutes.
#OLLAMA_LOAD_TIMEOUT='300'
## Allowed hosts for remote models (default "ollama.com")
#OLLAMA_REMOTES='ollama.com'
## LogLevel returns the log level for the application.
## Values are 0 or false INFO (Default), 1 or true DEBUG, 2 TRACE
#OLLAMA_DEBUG='1'
## FlashAttention enables the experimental flash attention feature.
# OLLAMA_FLASH_ATTENTION='false'
## Quantization type for the K/V cache (default: f16)
# OLLAMA_KV_CACHE_TYPE='f16'
##  NoHistory disables readline history.
# OLLAMA_NOHISTORY=''
## NoPrune disables pruning of model blobs on startup.
## OLLAMA_NOPRUNE=''
## SchedSpread allows scheduling models across all GPUs.
# OLLAMA_SCHED_SPREAD=''
## MultiUserCache optimizes prompt caching for multi-user scenarios
# OLLAMA_MULTIUSER_CACHE=''
## Enable the new Ollama engine
# OLLAMA_NEW_ENGINE=''
## ContextLength sets the default context length
# OLLAMA_CONTEXT_LENGTH='4096'
## Auth enables authentication between the Ollama client and server
# OLLAMA_AUTH=''
## Enable Vulkan backend
# OLLAMA_VULKAN=''
## NumParallel sets the number of parallel model requests. NumParallel can be configured via the OLLAMA_NUM_PARALLEL environment variable.
# OLLAMA_NUM_PARALLEL='1'
## MaxRunners sets the maximum number of loaded models. MaxRunners can be configured via the OLLAMA_MAX_LOADED_MODELS environment variable.
# OLLAMA_MAX_LOADED_MODELS='0'
## MaxQueue sets the maximum number of queued requests. MaxQueue can be configured via the OLLAMA_MAX_QUEUE environment variable.
# OLLAMA_MAX_QUEUE='512'
## Set LLM library to bypass autodetection
# OLLAMA_LLM_LIBRARY=''
